{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c37595e",
   "metadata": {},
   "source": [
    "# Coding question for Guided Protein Diffusion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0490bea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\alexa\\anaconda3\\envs\\evodiff\\lib\\site-packages\\evodiff\\pretrained.py:2: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "import evodiff\n",
    "import scipy.spatial.distance as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09d123a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sohl-dickstein\n"
     ]
    }
   ],
   "source": [
    "from evodiff.pretrained import D3PM_UNIFORM_38M\n",
    "\n",
    "checkpoint = D3PM_UNIFORM_38M(return_all=True)\n",
    "model, collater, tokenizer, scheme, timestep, Q_bar, Q = checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "7142686c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [01:01<00:00,  8.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final seq ['MNNRVKGDVLLNSQLLKYRELAEDCQLTAYTTSDQQRHPWFTLLREQVRTLTVGRTLARNLSGIDEALVTTARRDGRTQIVVATSAESKSRWRSLAGSSR']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from evodiff.generate import generate_d3pm\n",
    "\n",
    "seq_len = 100 \n",
    "\n",
    "tokenized_sample, generated_sequence = generate_d3pm(model, tokenizer, Q, Q_bar, timestep, seq_len, batch_size=1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08765f1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def d(x, y): #Hamming distance\n",
    "    assert len(x) == len(y)\n",
    "    return sum(xi != yi for xi, yi in zip(x, y))\n",
    "\n",
    "def reward(x):\n",
    "    return 4 - d(x[:4], 'MSTQ')\n",
    "\n",
    "\n",
    "reward('MSAAAAAAAG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e4b23897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def generate_d3pm_FK(model, tokenizer, Q, Q_bar, timesteps, seq_len,frequency=0.1, batch_size=3, device='cuda',llambda=1,reward_fn=reward):\n",
    "    \"\"\"\n",
    "    Samples from the D3PM reverse process using a Feynman-Kac steering\n",
    "    \n",
    "    \"\"\"\n",
    "    n_samples = 50 #number of samples used to compute the intermediate rewards\n",
    "    K = tokenizer.K\n",
    "\n",
    "    sample = torch.randint(0, K, (batch_size, seq_len), dtype=torch.long, device=device)\n",
    "    Q = Q.to(device)\n",
    "    Q_bar = Q_bar.to(device)\n",
    "    \n",
    "    log_product_of_potentials = torch.zeros(batch_size, device=device)\n",
    "    max_R_phi = torch.zeros((batch_size), device=device)  \n",
    "    #sum_R_phi = torch.zeros((batch_size), device=device)\n",
    "    #diff_R_phi = torch.zeros((batch_size), device=device) \n",
    "    #R_phi = torch.zeros((batch_size), device=device) #Intermediate reward at previous timestep\n",
    "\n",
    "    \n",
    "    timesteps_iter = list(range(timesteps - 1, 0, -1))\n",
    "   \n",
    "    N = int(1/frequency)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for t in tqdm(timesteps_iter):\n",
    "            \n",
    "            timesteps_tensor = torch.full((batch_size,), t, device=device, dtype=torch.long)\n",
    "            \n",
    "            if t > 1 and (t % N == 0):\n",
    "                # Compute potential weights\n",
    "                w = torch.exp(llambda * max_R_phi)    \n",
    "                \n",
    "                #Resampling according to potentials\n",
    "                a_t = torch.multinomial(w, num_samples=batch_size, replacement=True)\n",
    "                #print(\"Resampling at time\", t)\n",
    "                #print(\"Weights\", w)\n",
    "                sample = sample[a_t]  \n",
    "                max_R_phi = max_R_phi[a_t]\n",
    "                #sum_R_phi = sum_R_phi[a_t]\n",
    "                #diff_R_phi = diff_R_phi[a_t]\n",
    "                #R_phi = R_phi[a_t]\n",
    "\n",
    "                log_product_of_potentials = log_product_of_potentials[a_t] + torch.log(w[a_t])\n",
    "            \n",
    "            # sampling x_{t-1})\n",
    "            prediction = model(sample, timesteps_tensor)\n",
    "            p = prediction[:, :, :K]  # p_theta_tilde (x_0_tilde | x_t)\n",
    "            p = torch.nn.functional.softmax(p, dim=-1).to(torch.float64)  # [B, L, K]\n",
    "            x_tminus1 = sample.clone()\n",
    "\n",
    "            \n",
    "            for i, s in enumerate(sample):\n",
    "                \n",
    "                x_t_b = tokenizer.one_hot(s)\n",
    "                if not isinstance(x_t_b, torch.Tensor):\n",
    "                    x_t_b = torch.tensor(x_t_b, device=device, dtype=torch.float64)\n",
    "                else:\n",
    "                    x_t_b = x_t_b.to(device=device, dtype=torch.float64)\n",
    "\n",
    "                A = torch.mm(x_t_b, torch.t(Q[t]))  # [L x K]\n",
    "                Q_expand = Q_bar[t-1].unsqueeze(0).expand(A.shape[0], K, K)  # [ L x K x K]\n",
    "                B_pred = p[i].unsqueeze(2) * Q_expand\n",
    "                q_t = A.unsqueeze(1) * B_pred  # [ L x K x K ]\n",
    "                p_theta_marg = torch.bmm(q_t.transpose(1,2), p[i].unsqueeze(2)).squeeze(-1)  # [L x K]\n",
    "                p_theta_marg = p_theta_marg / (p_theta_marg.sum(dim=1, keepdim=True))\n",
    "                \n",
    "                x_tminus1[i] = torch.multinomial(p_theta_marg, num_samples=1).squeeze(1)\n",
    "                # On final timestep pick next best from standard AA\n",
    "                if t == 1:\n",
    "                    x_tminus1[i] = torch.multinomial(p_theta_marg[:, :K-6], num_samples=1).squeeze(1)\n",
    "            \n",
    "            sample = x_tminus1\n",
    "            \n",
    "            #print(\"Sample at time\",t,[tokenizer.untokenize(s.to('cpu').long()) for s in sample])\n",
    "\n",
    "            # Compute intermediate rewards\n",
    "            if t > 1 and (t % N == 0):\n",
    "                # second forward for p(x0 | x_{t-1}) needed for intermediate reward estimation\n",
    "                prediction2 = model(sample, timesteps_tensor)\n",
    "                p2 = torch.nn.functional.softmax(prediction2[:, :, :K], dim=-1).to(torch.float64)\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    \n",
    "                    x_0_samples = torch.multinomial(p2[i], num_samples=n_samples, replacement=True)  # [L, n_samples]\n",
    "                    \n",
    "                    r_vals = []\n",
    "                    \n",
    "                    for j in range(x_0_samples.shape[1]):\n",
    "                        seq_tensor = x_0_samples[:, j].to('cpu').long()\n",
    "                        seq_str = tokenizer.untokenize(seq_tensor)\n",
    "                        r_vals.append(float(reward_fn(seq_str)))\n",
    "                    \n",
    "                    r_vals = torch.tensor(r_vals, dtype=torch.float64, device=device)\n",
    "                    \n",
    "                    r_phi =  torch.log(torch.mean(torch.exp(r_vals )) + 1e-12)\n",
    "\n",
    "                    #sum_R_phi[i] += r_phi\n",
    "                    #diff_R_phi[i] = r_phi - R_phi[i]\n",
    "                    #R_phi[i] = r_phi\n",
    "                    if r_phi > max_R_phi[i]:\n",
    "                        max_R_phi[i] = r_phi\n",
    "            \n",
    "            elif t == 1:\n",
    "                # Compute final rewards \n",
    "                final_rewards = torch.tensor([\n",
    "                    reward_fn(tokenizer.untokenize(sample[i].to('cpu').long())) \n",
    "                    for i in range(batch_size)\n",
    "                ], device=device, dtype=torch.float64)\n",
    "                \n",
    "                print(final_rewards)\n",
    "                # Compute corrected final weights\n",
    "                w = torch.softmax(llambda * final_rewards - log_product_of_potentials,dim=-1) \n",
    "\n",
    "                print(w)\n",
    "                \n",
    "                \n",
    "                #Final resampling\n",
    "                a_t = torch.multinomial(w, num_samples=batch_size, replacement=True)\n",
    "                sample = sample[a_t].clone()\n",
    "\n",
    "    untokenized = [tokenizer.untokenize(s.to('cpu').long()) for s in sample]\n",
    "    print(\"final seq\", untokenized)\n",
    "    return sample, untokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a72f2ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 499/499 [15:00<00:00,  1.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 2., 1., 2., 1., 1., 2., 1., 2., 1.,\n",
      "        1., 2., 1., 2., 2., 1., 1., 1., 2., 1., 1., 1.], dtype=torch.float64)\n",
      "tensor([0.0165, 0.0165, 0.0165, 0.0165, 0.0165, 0.0159, 0.0165, 0.0165, 0.0165,\n",
      "        0.0784, 0.0165, 0.0814, 0.0159, 0.0165, 0.0784, 0.0165, 0.0814, 0.0165,\n",
      "        0.0165, 0.0814, 0.0165, 0.0814, 0.0784, 0.0165, 0.0165, 0.0165, 0.0784,\n",
      "        0.0165, 0.0165, 0.0165], dtype=torch.float64)\n",
      "final seq ['MTDRLSVTVVLGAAMAATLLNAAAFSATRAWTHAVDVPRAITNAVTGTAPVSLYTSQLHEETTAWPPQTAMLSLLAESHAGRSASVVSTQRRVDMIDEVG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPETAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MTDRLLGTVVLGAAMAAVPDGAAHFSATRDWTHAVDVLRAITIAVTGTPPVVLYTSQLHEETFAWPPFTALLSLLTESSTRHSASVVRDQRRVGRDWEVG', 'MTWRLLLTVALGAAMAAPADAAALFSATRGVTVAVDVLPAITGAVTGTVPVSRYTSELPELAFAWPPQTAMLSLLHESGKLRSASVVETQRRGMFIREVG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPATAAWILAADVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPATAAGILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MTDRLLGTVVLGAAMAAVSDGAAHFSATRDWTHAVDVLRAITIAVTGTPPVVLYTSQLHEETFAWPPFTALLSLLTESSTRHSASVVRDQRRVGRDWEVG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPATAAWILAADVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPVTAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRMLWEYG', 'MTARLAWTVLLGAAMAAHADNAALNSAVRGDSHAVDLARAATNAVTGTVGPSLITSELHELTNAWPPQTAMLLLLWESHIVRSADVVRTQRRGMMIWEVG', 'MTRRLLVTVALGAAMAAGASAAALFSATRGVTCAVDVLPAITGAVTGTVPVSLYTSELPELTFAWPPQSAMLSLLWESGKTRSASVVETQRRGMFISEVG', 'MSVVLLAAVVLGAQTAAEVGQSAPVPETAAWILANDVPRAITLNVTDAGKVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDARRVRMLWEYG', 'MTRRLLVTVALGAAMAASASAAALFSATRGVTCAVDVLPAIAGAVTGTVPVSLYTSELPELTFAWPPQSAMLSLLWESGKPRSASVVETQRRGMFISEVG', 'MTDRLSVTVVLGAAMAATLLNAAAFSATRAWTHAVDVPRAITNAVTGTAPVSLYTSQLHEETTAWPPQTAMLSLLAESHAFRSASVVSTQRRVDMIDEVG', 'MTDRLSVTVVLGAAMAAFSLNAAMFSATRRWTHAVDVPRAITNAVTGTGPVSLYTSQLHEETTAWPPQTAMLSLLAESAKARSASVVSTQRRVDMIDEVG', 'MTRRLLVTVALGAAMAAGASAAALFSATRGVTCAVDVLPAITGAVTGTVPVSLYTSELPELTFAWPPQSAMLSLLWESGKTRSASVVETQRRGMFISEVG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPATAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPVPETAAWILANDVPRAITLNVTDAGKVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDARRVRMLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPVPETAAWILANDVPRAITLNVTDAGKVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDARRVRMLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPVTAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRMLWEYG', 'MTGRLDGTVVLGAAMAAQADGAAGFSATRQRTHAVDVLRAITDAVTGTPPVVLYTSLLHEETFAWPPFTALLAALQESSARRSASQVRDGRRVKRDWEVG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPATAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MTDRLLGTVVLGAAMAAVSDGAAHFSATRDWTHAVDVLRAITIAVTGTPPVVLYTSQLHEETFAWPPFTALLSLLTESSTRHSASVVRDQRRVGRDWEVG', 'MTDRLLGTVVLGAAMAAVVDGAAHFSATRDWTHAVDVLRAITNAVTGTPPVVLYTSQLHEETFAWPPFTALLSLLTESSKRHSASVVRDQRRVGRDWEVG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPVTAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRMLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPVTAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRMLWEYG', 'MTGRLDGTVVLGAAMAAQADGAAGFSATRQRTHAVDVLRAITDAVTGTPPVVLYTSLLHEETFAWPPFTALLAALQESSARRSASQVRDGRRVKRDWEVG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPATAAGILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPGPATAAWILANDVPRAITLNVTDAGTVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDYRRVRPLWEYG', 'MSVVLLAAVVLGAQTAAEVGQSAPVPETAAWILANDVPRAITLNVTDAGKVHLWHVGLHHLEWANVHVTGLELEYGTEAKDGSAQVMYDARRVRMLWEYG']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "seq_len = 100\n",
    "\n",
    "tokenized_sample, generated_sequence = generate_d3pm_FK(model, tokenizer, Q, Q_bar, timestep, seq_len, batch_size=30, device='cpu',frequency=0.1,llambda=3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "40ef9c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "rewards = [reward(seq) for seq in generated_sequence]\n",
    "print(max(rewards))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evodiff",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
